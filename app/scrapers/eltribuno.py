from .base import BaseScraper
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
import datetime
import re
from urllib.parse import urljoin
import time

class ElTribunoScraper(BaseScraper):
    BASE_URL = "https://eltribunodejujuy.com/seccion/policiales"

    def __init__(self, fecha_limite=None):
        super().__init__(fecha_limite)
        self.media_id = 'eltribuno'

    def scrape(self, db) -> int:
        noticias_guardadas = 0
        pagina = 1
        seguir = True

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        while seguir:
            url_pagina = f"{self.BASE_URL}/{pagina}" if pagina > 1 else self.BASE_URL
            print(f"üìÑ Scraping p√°gina {pagina}: {url_pagina}")
            
            try:
                response = requests.get(url_pagina, headers=headers, timeout=10)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, "html.parser")
                
                # Encuentra todos los art√≠culos en la p√°gina
                items = soup.find_all('article') or soup.find_all(class_='article-item')
                
                if not items:
                    print(f"ü§∑ No se encontraron m√°s art√≠culos en la p√°gina {pagina}")
                    break

                found_articles = False
                
                for item in items:
                    try:
                        # Obtiene el enlace y t√≠tulo
                        link_element = item.find('a')
                        if not link_element:
                            continue
                            
                        url = urljoin(self.BASE_URL, link_element['href'])
                        
                        # Extrae la fecha del enlace
                        # Formato esperado: .../2025-1-29-0-31-0-titulo-de-la-noticia
                        date_parts = url.split('/')[-1].split('-')[:3]
                        if len(date_parts) < 3:
                            continue
                            
                        article_date = datetime.date(int(date_parts[0]), int(date_parts[1]), int(date_parts[2]))
                        
                        # Si el art√≠culo es anterior a la fecha l√≠mite, terminar
                        if self.fecha_limite and article_date < self.fecha_limite:
                            print(f"üìÖ Se alcanz√≥ la fecha l√≠mite ({self.fecha_limite}), finalizando b√∫squeda")
                            seguir = False
                            break
                        
                        found_articles = True
                        
                        # Obtiene el t√≠tulo
                        title_element = item.find('h2') or item.find('h3') or item.find(class_='title')
                        if not title_element:
                            continue
                            
                        titulo = title_element.text.strip()
                        
                        # Scrapea la p√°gina individual del art√≠culo para m√°s detalles
                        try:
                            print(f"üîç Scrapeando art√≠culo: {url}")
                            nota_resp = requests.get(url, headers=headers, timeout=10)
                            nota_resp.raise_for_status()
                            nota_soup = BeautifulSoup(nota_resp.text, "html.parser")
                            
                            contenido, contenido_crudo = self._extract_content(nota_soup)

                            # Construir la fecha desde las partes extra√≠das de la URL
                            fecha = article_date
                            noticia = {
                                "titulo": titulo,
                                "contenido": contenido,
                                "contenido_crudo": contenido_crudo,
                                "fecha": fecha,
                                "url": url,
                                "media_id": self.media_id
                            }
                            # Guardar en la base de datos
                            from app.db import Noticia
                            noticia_obj = Noticia(
                                titulo=noticia["titulo"],
                                contenido=noticia["contenido"],
                                contenido_crudo=noticia["contenido_crudo"],
                                fecha=noticia["fecha"],
                                url=noticia["url"],
                                media_id=noticia["media_id"]
                            )
                            db.add(noticia_obj)
                            db.commit()
                            noticias_guardadas += 1
                            print(f"‚úÖ Art√≠culo extra√≠do y guardado: {titulo}")
                            # Espera entre requests de art√≠culos individuales
                            time.sleep(2)
                            
                        except Exception as e:
                            print(f"‚ùå Error scraping art√≠culo individual {url}: {str(e)}")
                            continue
                    
                    except Exception as e:
                        print(f"‚ùå Error procesando art√≠culo: {str(e)}")
                        continue
                
                if not found_articles:
                    print(f"ü§∑ No se encontraron art√≠culos v√°lidos en la p√°gina {pagina}")
                
                pagina += 1
                time.sleep(3)  # Espera entre p√°ginas
                
            except Exception as e:
                print(f"‚ùå Error scraping El Tribuno: {e}")
                break
                
        return noticias_guardadas

    def _extract_content(self, soup):
        """Extrae el contenido limpio y el HTML crudo del art√≠culo."""
        try:
            article = soup.find('article')
            if article:
                raw_html = str(article)
                paragraphs = article.find_all('p')
                clean_text = "\n\n".join([p.get_text().strip() for p in paragraphs])
                return clean_text, raw_html
            return "", ""
        except Exception as e:
            print(f"‚ö†Ô∏è  Error extrayendo contenido: {e}")
            return "", "" 